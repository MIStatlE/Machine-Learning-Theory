# 关于我

大家好，我是 **MIStatlE** 🎓  
目前研究生就读于 **复旦大学**，研究方向聚焦 **机器学习理论**。  

在 **小红书、知乎、微信公众号、B 站** 等平台，我均使用同名账号 **@MIStatlE**。  
我将持续整理学习过程中遇到的高价值书籍、笔记与课程，构建一座**小而精的资源库**，希望为你的学习之旅提供助力与灵感。🚀


# 📚 机器学习理论导论

机器学习理论（Machine Learning Theory, **MLT**）致力于回答这样一些**根本问题**：

| 典型问题 | 示例 |
|-----------|-------|
| **可学性** | 什么样的问题可以通过有限数据被算法**可靠学习**？（PAC 学习、VC 维） |
| **样本复杂度** | 要达到给定误差，需要 **多少样本**？（一致收敛、Rademacher 复杂度） |
| **算法复杂度** | 存在 **高效**（多项式时间）同时又统计最优的算法吗？（信息论下界 vs. 算法上界） |
| **泛化能力** | 为什么深度网络在过参数化下仍能 **泛化**？（隐式正则、平坦最小化） |
| **探索与利用** | 在交互式环境中，如何权衡 **试探**与 **收益**？（Multi-Armed Bandit、强化学习后悔界） |

> **一句话**：MLT 关心「什么能学、学多少、多久学到、用什么学」——并用**数学证明**给出答案。

---

## 🗺️ 学习路径总览

| 阶段 | 建议课程 | 目标与能力 |
|------|----------|------------|
| **Ⅰ. 数学基础** | 高等数学 + 线性代数 + 概率统计 → 现代概率论 ➕ 优化理论 | 搭建分析与证明工具箱 |
| **Ⅱ. 核心理论** | 统计学习理论 → 高维概率 → 凸优化与对偶 → 泛化与复杂度 | 掌握 PAC/VC、Rademacher、信息论等关键框架 |
| **Ⅲ. 高级专题** | 强化学习理论 → 在线/Bandit 学习 → 深度学习理论 | 站在前沿阅读顶会 / 论文，理解开放问题 |
| **Ⅳ. 研究实践** | 研读 COLT / NeurIPS / ICML 理论论文 | 能独立追踪 & 评价最新进展 |

---

## 🔗 先修要求（Prerequisites）
本资源库适合已经学过高等数学、线性代数、概率论与数理统计的本科生、研究生

| 课程 | Books | Notes / Slides | MOOCs | Prerequisite | 在 MLT 中的作用 |
|------|-------|----------------|-------|--------------|-----------------|
| **线性代数** | *Matrix Analysis* — Horn & Johnson；*Linear Algebra Done Right* — Axler | MIT 18.06 笔记 | MIT OpenCourseWare 18.06；3Blue1Brown *Essence of LA* | 高中数学 | 向量空间、特征分解、奇异值是泛化与优化分析的基石 |
| **概率论与统计** | *Probability Theory* — Gut；*Statistical Inference* — Casella & Berger | CMU 36-705 讲义 | Harvard *Stat 110* | 微积分 | 建立随机变量、极限定理直觉，为泛化与上界/下界奠基 |
| **高等数学** | *同济大学第七版*  | - | - | 微积分 | 掌握极限、测度，支撑严谨证明（如可测性假设） |


---

## 🛠 该资源库的定位

| 目标 | 说明 |
|------|------|
| **体系化地图** | 按阶段梳理必备知识块，帮助快速定位盲区 |
| **课程索引** | 对每门课给出作用说明和先修要求，便于规划学习顺序 |
| **轻量化入口** | 提供大纲与指北，请按照需求选择适合自己的资料 |



> ✨ **愿景**：搭建中文世界面向研究者的机器学习理论“索引站”，让每位学习者都能站在清晰的坐标系上启程。



> 每个子目录下包含
> • 📚 **README/** – 对本课程的定位详解以及资源评价
> • 📚 **books/** – 官方 PDF/合法电子稿  
> • 📝 **notes/** – Markdown/Jupyter 笔记  
> • 🔗 **links.md** – 高质量外部资源索引  




